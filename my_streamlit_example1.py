import streamlit as st
from dotenv import load_dotenv
from langchain_fireworks import ChatFireworks
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import (
    PromptTemplate,
    FewShotPromptTemplate
)
from langchain_core.output_parsers import StrOutputParser
import re
from operator import itemgetter
from langchain_core.runnables import RunnableLambda
from langchain.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate
)
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s %(levelname)s %(message)s',
    level=logging.INFO,
    handlers=[
        logging.StreamHandler()
    ]
)
# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# éƒ¨ç½²åˆ°streamlitæ—¶ï¼Œè¯·åœ¨streamlitä¸­é…ç½®ç¯å¢ƒå˜é‡
load_dotenv()

class JobAssistant:
    def __init__(self, model_path="accounts/fireworks/models/llama-v3-70b-instruct", 
                temperature=0.3,
                top_p=0.3):
        self.llm = ChatFireworks(model=model_path, temperature=temperature)
        self.response_dict = {
            "ç¦»èŒåŸå› ": {
                "response": "æœ‰æ¢å·¥æ„æ„¿ï¼Œä¸Šå®¶å…¬å¸ç¦»æˆ‘å±…ä½åœ°å¤ªè¿œï¼Œé€šå‹¤æ—¶é—´å¤ªé•¿ã€‚",
                "examples": [{"text": "ç¦»èŒ/æ¢å·¥ä½œçš„åŸå› ","label": "ç¦»èŒåŸå› "}]
            },
            "è–ªèµ„": {
                "response": "æˆ‘æœŸæœ›è–ªèµ„ä¸º30Kï½40Kã€‚",
                "examples": [{"text": "ä½†æ˜¯æˆ‘ä»¬åº”è¯¥æœ€é«˜30Kï¼Œä¸€èˆ¬è¿˜è¾¾ä¸åˆ°.","label": "è–ªèµ„"}]
            },
            "å¤–åŒ…&å¤–å&å¤–æ´¾&é©»åœº": {
                "response": "è¯·å‘é€æˆ–è¯´æ˜èŒä½çš„åŠå…¬åœ°ç‚¹å®šä½ã€‚ä»¥åŠè–ªèµ„èŒƒå›´ã€‚æˆ‘æœŸæœ›è–ªèµ„èŒƒå›´30-40Kï¼Ÿ",
                "examples": [{"text": "ä½ å¥½ï¼Œæˆ‘ä»¬æ˜¯å¤–åå²—ä½ï¼Œåœ¨å›½å®¶ç”µç½‘ å—ç‘å·¥ä½œçš„","label": "å¤–åŒ…&å¤–å&å¤–æ´¾&é©»åœº"}]
            },
            "å…¼èŒ": {
                "response": "èŒä½çš„åŠå…¬åœ°ç‚¹åœ¨å“ªï¼Ÿè–ªèµ„å¤šå°‘ï¼Œæ€ä¹ˆç»“ç®—ï¼Ÿ",
                "examples": [{"text": "å“ˆå–½ï½æœ¬èŒä½ä¸ºçº¿ä¸Šå…¼èŒï¼Œä¸€å•ä¸€ç»“æ¬¾ï¼Œæ ¹æ®è‡ªå·±æ—¶é—´è‡ªç”±æ¥å•ï¼Œä¸è€½è¯¯è‡ªå·±çš„ä¸»ä¸šï¼Œæ‚¨çœ‹æ„Ÿå…´è¶£å˜›ï¼Ÿ","label":"å…¼èŒ"}]
            },
            "é¢„çº¦é¢è¯•": {
                "response": "æœ¬å‘¨å†…ä¸Šåˆã€ä¸‹åˆéƒ½æœ‰æ—¶é—´ã€‚",
                "examples": [{"text": "æƒ³çº¦æ‚¨é¢è¯•ï¼Œæ–¹ä¾¿çš„è¯éº»çƒ¦å‘Šè¯‰æˆ‘ä¸€ä¸‹æ‚¨å¯ä»¥çº¦é¢è¯•çš„æ—¥æœŸåŠæ—¶é—´ã€è¯·é€‰æ‹©å·¥ä½œæ—¥å†…çš„ä¸Šåˆ10-12ç‚¹æˆ–ä¸‹åˆ14ç‚¹åˆ°17ç‚¹å†…çš„æ—¶é—´ã€‘ã€‚","label":"é¢„çº¦é¢è¯•"}]
            },
            "åˆ°å²—æ—¶é—´": {
                "response": "ä¸¤å‘¨å†…åˆ°å²—ã€‚",
                "examples": [{"text": "å’±åˆ°å²—æ—¶é—´å‘¢ã€‚","label":"åˆ°å²—æ—¶é—´"}]
            },
            "å…¶ä»–": {
                "response": "",
                "examples": []
            }
        }

        self.examples = []
        for key in self.response_dict:
            r_examples = self.response_dict[key]["examples"]
            if len(r_examples) > 0:
                self.examples.extend(r_examples)

        self.example_prompt = PromptTemplate.from_template(
            """æ–‡æœ¬: {text}
            ç±»åˆ«: {label}
            """
        )

        self.prefix = f"""
        ç»™å‡ºæ¯ä¸ªæ–‡æœ¬çš„ç±»åˆ«ï¼Œç±»åˆ«åªèƒ½å±äºä»¥ä¸‹åˆ—å‡ºçš„ä¸€ç§

        {"- ".join(self.response_dict.keys())}

        å¦‚æœä¸å±äºä»¥ä¸Šç±»åˆ«ï¼Œåˆ™ç±»åˆ«åç§°ä¸ºâ€œå…¶ä»–â€ã€‚

        ä¾‹å¦‚ï¼š
        """

        self.suffix = """æ–‡æœ¬: {input}\nç±»åˆ«:
        """

        self.few_shot_prompt = FewShotPromptTemplate(
            examples=self.examples,
            example_prompt=self.example_prompt,
            prefix=self.prefix,
            suffix=self.suffix,
            input_variables=["input"],
            example_separator="\n"
        )

        self.chain = self.few_shot_prompt | self.llm | StrOutputParser()

        self.system_message_prompt = SystemMessagePromptTemplate.from_template("ä½ æ˜¯æ±‚èŒåŠ©æ‰‹äºå…ˆç”Ÿï¼Œç”¨æ±‰è¯­äº¤æµã€‚")
        self.human_message_prompt = HumanMessagePromptTemplate.from_template("HRé—®æˆ–è¯´: â€œ{question}â€ã€‚{response}ä½ ç”¨æ±‰è¯­å›ç­”: ")
        self.prompt = ChatPromptTemplate.from_messages(
            [self.system_message_prompt, self.human_message_prompt])

        self.final_chain = {"question": itemgetter("input"),
                            "response": itemgetter("input") | RunnableLambda(self.question_classify)} | \
                           self.prompt | self.llm | StrOutputParser()

    def question_classify(self, text):
        label = ""
        text = text.strip()
        if len(text) > 0:
            label = self.chain.invoke({"input": text})
            label = re.sub('ç±»åˆ«: ?', '', label)
        label = label if label in self.response_dict else "å…¶ä»–"
        logger.info(f"é—®é¢˜ç±»åˆ«: {label}")
        response = self.response_dict[label]["response"]
        if len(response)>0:
            response = f"ä½ åœ¨å›ç­”ä¸­ä½“ç°ä¸€ä¸‹å†…å®¹: {response}" 
        logger.info(f"é—®é¢˜åˆ†ç±»å“åº”: {response}")
        return response

    def get_response(self, text):
        response = self.final_chain.invoke({"input":text})
        response = re.sub(r'^(["ã€Œâ€œ])(.+?)(["ã€â€])$', 
                      lambda m: m.group(2) if (m.group(1) == m.group(3) or 
                                               (m.group(1) == '"' and m.group(3) == '"') or 
                                               (m.group(1) == 'ã€Œ' and m.group(3) == 'ã€') or 
                                               (m.group(1) == 'â€œ' and m.group(3) == 'â€')) 
                      else m.group(0), response)
        return response

# ä½¿ç”¨ç¤ºä¾‹
assistant = JobAssistant()

# é¡µé¢å¤§æ ‡é¢˜
st.title("ä¸ªäººæ±‚èŒåŠ©æ‰‹")
st.title("ğŸ’¬ èŠå¤©æœºå™¨äºº")
# é¡µé¢æè¿°
st.caption("ğŸš€ ä¸€ä¸ªStreamlitä¸ªäººæ±‚èŒåŠ©æ‰‹èŠå¤©æœºå™¨äººï¼ŒåŸºäºFireWorksçš„llama-v3-70b-instructæ¨¡å‹")
# ä¾§è¾¹æ 
with st.sidebar:
    st.write("ä»€ä¹ˆä¹Ÿä¸æƒ³å†™")
    
# åˆå§‹åŒ–èŠå¤©æ¶ˆæ¯ä¼šè¯
if "messages" not in st.session_state:
    #  æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
    st.session_state["messages"] = [{"role": "assistant", "content": "æˆ‘æ˜¯ä½ çš„ä¸ªäººæ±‚èŒåŠ©æ‰‹ï¼Œå¸®ä½ å›ç­”HRæå‡ºçš„é—®é¢˜ï¼Œä½ å¯ä»¥å°†HRçš„é—®é¢˜è¾“å…¥ç»™æˆ‘ï¼"}]

# æ˜¾ç¤ºä¼šè¯ä¸­çš„æ‰€æœ‰èŠå¤©æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# èŠå¤©è¾“å…¥è¡¨æ ¼
# è¿™å¥ä»£ç ä½¿ç”¨äº†æµ·è±¡è¿ç®—ç¬¦ï¼Œå°†ç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥çš„å†…å®¹èµ‹å€¼ç»™å˜é‡promptï¼Œå¹¶æ£€æŸ¥è¿™ä¸ªè¾“å…¥å†…å®¹æ˜¯å¦ä¸ºçœŸï¼ˆå³æ˜¯å¦æœ‰è¾“å…¥å†…å®¹ï¼‰ã€‚
if prompt := st.chat_input("HRçš„é—®é¢˜"):
    logger.info(f"ç”¨æˆ·è¾“å…¥: {prompt}")
    # å‘ä¼šè¯æ¶ˆæ¯ä¸­æ·»åŠ ç”¨æˆ·è¾“å…¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.chat_message("user").write(prompt)
    # è°ƒç”¨é“¾è·å–å“åº”
    response = assistant.get_response(prompt)
    logger.info(f"AIå“åº”: {response}")
    # å‘ä¼šè¯æ¶ˆæ¯ä¸­æ·»åŠ åŠ©æ‰‹è¾“å…¥
    st.session_state.messages.append({"role": "assistant", "content": response})
    # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯
    st.chat_message("assistant").write(response)
