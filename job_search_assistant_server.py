import streamlit as st
from dotenv import load_dotenv
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter
from langchain.document_loaders import WebBaseLoader
from langchain_fireworks import FireworksEmbeddings, ChatFireworks
from langchain.vectorstores import FAISS
from langchain_core.prompts import (
    FewShotPromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate,
    PromptTemplate)
from langchain_core.runnables import (
    RunnableParallel,
    RunnablePassthrough,
    RunnableLambda)
from langchain_core.output_parsers import StrOutputParser
import logging
import re
import os
from langchain_community.embeddings import HuggingFaceBgeEmbeddings

# é…ç½®æ—¥å¿—
logging.basicConfig(
    format='%(asctime)s %(levelname)s %(message)s',
    level=logging.INFO,
    handlers=[
        logging.StreamHandler()
    ]
)
# è·å–æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)
 # éƒ¨ç½²åˆ°streamlitæ—¶ï¼Œè¯·åœ¨streamlitä¸­é…ç½®ç¯å¢ƒå˜é‡
load_dotenv()
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "job-search-assistant-webpage"
logging.info(f'LANGCHAIN_TRACING_V2: {os.getenv("LANGCHAIN_TRACING_V2", "æœªè®¾ç½®")}')
logging.info(f'LANGCHAIN_ENDPOINT: {os.getenv("LANGCHAIN_ENDPOINT", "æœªè®¾ç½®")}')
logging.info(f'LANGCHAIN_PROJECT: {os.getenv("LANGCHAIN_PROJECT", "æœªè®¾ç½®")}')

class JobSearchAssistant:
    def __init__(self, url, embedding_model_name, chat_model_name):
        # æ–‡æ¡£åŠ è½½ã€åˆ†å‰²
        self.loader = WebBaseLoader(url)
        self.raw_documents = self.loader.load()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=100,
            chunk_overlap=20,
            length_function=len,
            add_start_index=False,
        )
        self.documents = self.text_splitter.split_documents(self.raw_documents)
        print(f"åˆ†å‰²åå¿«æ•°: {len(self.documents)}")

        # å‘é‡åŒ–ã€å­˜å‚¨
        if embedding_model_name == "BAAI/bge-large-zh-v1.5":
            model_kwargs = {"device": "cpu"}
            encode_kwargs = {"normalize_embeddings": True}
            self.embedding_model = HuggingFaceBgeEmbeddings(
                model_name=embedding_model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
            )
        else:
            self.embedding_model = FireworksEmbeddings(model=embedding_model_name)
        self.db = FAISS.from_documents(
            documents=self.documents, embedding=self.embedding_model)
        print(f"ç´¢å¼•ç‰‡æ®µæ•°: {self.db.index.ntotal}")

        # æ£€ç´¢å™¨
        self.retriever = self.db.as_retriever()

        # æ£€ç´¢é“¾
        self.question_retrieval_chain = RunnableLambda(lambda x:x["input"]) | self.retriever | RunnableLambda(
            lambda docs: "\n".join([doc.page_content for doc in docs]))

        # å®ä¾‹åŒ–èŠå¤©æ¨¡å‹
        self.chat = ChatFireworks(
            model=chat_model_name, temperature=0.3, top_p=0.3)

        # åˆ†ç±»è¯å…¸
        self.question_classify_dict = {
            "ç¦»èŒåŸå› ": {
                "response": "æœ‰æ¢å·¥æ„æ„¿ï¼Œä¸Šå®¶å…¬å¸ç¦»æˆ‘å±…ä½åœ°å¤ªè¿œï¼Œé€šå‹¤æ—¶é—´å¤ªé•¿ã€‚",
                "examples": [{"text": "ç¦»èŒ/æ¢å·¥ä½œçš„åŸå› ", "label": "ç¦»èŒåŸå› "}]
            },
            "è–ªèµ„&è–ªèµ„èŒƒå›´&æœŸæœ›è–ªèµ„": {
                "response": "ä¸Šä¸€å®¶å•ä½è–ªèµ„ä¸º20K*15ï¼Œå½“å‰æˆ‘æœŸæœ›è–ªèµ„ä¸º30Kï½40Kã€‚",
                "examples": [{"text": "ä½†æ˜¯æˆ‘ä»¬åº”è¯¥æœ€é«˜30Kï¼Œä¸€èˆ¬è¿˜è¾¾ä¸åˆ°.", "label": "è–ªèµ„&è–ªèµ„èŒƒå›´&æœŸæœ›è–ªèµ„"}]
            },
            "å¤–åŒ…&å¤–å&å¤–æ´¾&é©»åœº": {
                "response": "è¯·å‘é€æˆ–è¯´æ˜èŒä½çš„åŠå…¬åœ°ç‚¹å®šä½ã€‚ä»¥åŠè–ªèµ„èŒƒå›´ã€‚æˆ‘æœŸæœ›è–ªèµ„èŒƒå›´50kä»¥ä¸Šã€‚",
                "examples": [{"text": "ä½ å¥½ï¼Œæˆ‘ä»¬æ˜¯å¤–åå²—ä½ï¼Œåœ¨å›½å®¶ç”µç½‘ å—ç‘å·¥ä½œçš„", "label": "å¤–åŒ…&å¤–å&å¤–æ´¾&é©»åœº"}]
            },
            "å…¼èŒ": {
                "response": "è¯·å‘é€æˆ–è¯´æ˜èŒä½çš„åŠå…¬åœ°ç‚¹å®šä½ã€‚ä»¥åŠè–ªèµ„èŒƒå›´ã€‚æˆ‘æœŸæœ›è–ªèµ„èŒƒå›´50kä»¥ä¸Šã€‚",
                "examples": [{"text": "å“ˆå–½ï½æœ¬èŒä½ä¸ºçº¿ä¸Šå…¼èŒï¼Œä¸€å•ä¸€ç»“æ¬¾ï¼Œæ ¹æ®è‡ªå·±æ—¶é—´è‡ªç”±æ¥å•ï¼Œä¸è€½è¯¯è‡ªå·±çš„ä¸»ä¸šï¼Œæ‚¨çœ‹æ„Ÿå…´è¶£å˜›ï¼Ÿ", "label": "å…¼èŒ"}]
            },
            "é¢„çº¦é¢è¯•": {
                "response": "è¯·æ‚¨ç¨ç­‰ï¼Œæˆ‘çœ‹ä¸€ä¸‹æˆ‘çš„æ—¶é—´ã€‚",
                "examples": [{"text": "æƒ³çº¦æ‚¨é¢è¯•ï¼Œæ–¹ä¾¿çš„è¯éº»çƒ¦å‘Šè¯‰æˆ‘ä¸€ä¸‹æ‚¨å¯ä»¥çº¦é¢è¯•çš„æ—¥æœŸåŠæ—¶é—´ã€‚", "label": "é¢„çº¦é¢è¯•"}]
            },
            "ä¼šè®®é‚€è¯·": {
                "response": "è¯·æ‚¨ç¨ç­‰ï¼Œæˆ‘çœ‹ä¸€ä¸‹æˆ‘çš„æ—¶é—´ã€‚",
                "examples": [{"text": "ä¼šè®®ä¸»é¢˜ï¼šä¸­ç”µä¿¡æ•°æ™ºç§‘æŠ€ä¼šè®®æ—¶é—´ï¼šä¸­å›½æ ‡å‡†æ—¶é—´ - åŒ—äº¬ç‚¹å‡»é“¾æ¥å…¥ä¼šï¼Œï¼Œæ‰“å¼€æ‰‹æœºè…¾è®¯ä¼šè®®å³å¯å‚ä¸", "label": "ä¼šè®®é‚€è¯·"}]
            },
            "åˆ°å²—æ—¶é—´": {
                "response": "ä¸¤å‘¨å†…åˆ°å²—ã€‚",
                "examples": [{"text": "å’±åˆ°å²—æ—¶é—´å‘¢ã€‚", "label": "åˆ°å²—æ—¶é—´"}]
            },
            "ç®€å†": {
                "response": "é©¬ä¸Šå‘é€ç®€å†ï¼Œä½†å¦‚æœåªæ˜¯æƒ³è·å–è”ç³»æ–¹å¼ï¼Œå°±ä¸è¦è€½è¯¯å¤§å®¶æ—¶é—´ã€‚",
                "examples": [{"text": "æ‚¨å¥½ æ–¹ä¾¿å‘ä¸€ä»½ç®€å†è¿‡æ¥å—ï¼Ÿ", "label": "ç®€å†"}]
            },
            "è¯·æ±‚å¾®ä¿¡": {
                "response": "è¯·ç›´æ¥åœ¨bossä¸Šç¡®å®šé¢è¯•æ—¶é—´ï¼Œå‘é€é¢„çº¦é¢è¯•è¯·æ±‚ã€‚",
                "examples": [{"text": "ä½ å¥½ æ–¹ä¾¿åŠ ä¸ªå¾®ä¿¡å—ï¼Ÿ", "label": "è¯·æ±‚å¾®ä¿¡"}]
            },
            "å…¶ä»–": {
                "response": "",
                "examples": []
            }
        }

        # æ„å»ºåˆ†ç±»æç¤º
        self.examples, self.example_prompt, self.prefix, self.suffix = self.prepare_question_classify_prompt()
        self.few_shot_prompt = FewShotPromptTemplate(
            examples=self.examples,
            example_prompt=self.example_prompt,
            prefix=self.prefix,
            suffix=self.suffix,
            input_variables=["input"],
            example_separator="\n"
        )

        # åˆ†ç±»é“¾
        self.question_classify_chain = self.few_shot_prompt | self.chat | StrOutputParser(
        ) | RunnableLambda(self.label_to_response)

        # ç³»ç»Ÿæç¤º
        self.system_message_prompt = SystemMessagePromptTemplate.from_template(f"""
        ä½ æ˜¯ä¸€ä¸ªæ±‚èŒåŠ©æ‰‹ï¼Œä»£è¡¨ â€œäºå…ˆç”Ÿâ€ å›ç­”HRé—®é¢˜ã€‚
        
        ä»¥ä¸‹æ˜¯äºå…ˆç”Ÿä¸ªäººä¿¡æ¯ï¼Œä»¥ä¸‹è¿™äº›ä¿¡æ¯åªæœ‰è¢«é—®åˆ°æ—¶æ‰è¾“å‡ºã€‚
        ```
        å·¥ä½œç»å†ï¼š
        - ä¸­å›½ç§‘å­¦é™¢ä¿¡æ¯å·¥ç¨‹ç ”ç©¶æ‰€ 2018å¹´8æœˆè‡³2024å¹´5æœˆ è‡ªç„¶è¯­è¨€å¤„ç†å·¥ç¨‹å¸ˆ è´Ÿè´£çŸ­æ–‡æœ¬åˆ†ç±»  ä½¿ç”¨pyhton/Javaè¯­è¨€ ç¦»èŒã€‚
        - è‹å®æ˜“è´­ 2017å¹´8æœˆè‡³2018å¹´7æœˆ è‡ªç„¶è¯­è¨€å¤„ç†å·¥ç¨‹å¸ˆ è´Ÿè´£å•†å“æ ‡é¢˜åˆ†ç±»å’Œå±æ€§è¯æŠ½å– ä½¿ç”¨C/C++è¯­è¨€ ç¦»èŒã€‚
        - åŒæ–¹çŸ¥ç½‘ 2015å¹´3æœˆè‡³2017å¹´7æœˆ è®¡ç®—è¯­è¨€å­¦å·¥ç¨‹å¸ˆ è´Ÿè´£è®ºæ–‡æŠ„è¢­æ£€æµ‹ç®—æ³•è®¾è®¡å®ç°ã€OpenCVç›¸ä¼¼å›¾åƒæ£€ç´¢ ä½¿ç”¨C/C++è¯­è¨€ ç¦»èŒã€‚
        ç°å±…ä½åœ°ï¼šåŒ—äº¬ã€‚
        æœŸæœ›å·¥ä½œåœ°ï¼šé•¿æœŸbaseåŒ—äº¬ï¼Œä¸æ¥å—å‡ºå·®ï¼ŒåŒ—äº¬å¹¿æ¸ é—¨é™„è¿‘æˆ–åœ°é“æ–¹ä¾¿ï¼Œé€šå‹¤æ—¶é—´1å°æ—¶ä»¥å†…ã€‚
        æ•™è‚²èƒŒæ™¯ï¼šåŒ—äº¬ä¿¡æ¯ç§‘æŠ€å¤§å­¦ ç¡•å£«/æœ¬ç§‘ã€‚
        è”ç³»æ–¹å¼ï¼šè¯·é—®æˆ‘çš„ä¸»äººã€‚
        æœŸæœ›èŒä½ï¼šè‡ªç„¶è¯­è¨€å¤„ç†(NLP)ã€å¤§æ¨¡å‹ã€‚
        å¯è®¿é—®æ±‚èŒåŠ©æ‰‹APPï¼šhttps://baiziyuandyufei-job-search-a-job-search-assistant-server-zv2qqt.streamlit.app/ã€‚
        ```
        """)

        # äººå·¥æç¤º
        self.human_message_prompt = HumanMessagePromptTemplate.from_template("""HRé—®æˆ–è¯´: {question}ã€‚\n\n{context}\n\nè¯·ç”¨æ±‰è¯­å›å¤å†…å®¹ï¼Œå†…å®¹çš„å¤´éƒ¨å’Œå°¾éƒ¨ä¸è¦å‡ºç°å¼•å·ã€‚""")

        # æ•´ä½“é“¾
        ## è¾“å…¥çš„å­—å…¸çš„å˜é‡åå« question
        ## RunnablePassthrough æ¥æ”¶è¾“å…¥å­—å…¸{"question":"ä½ å¥½"} è¾“å‡º {'question': 'ä½ å¥½', 'context':'xxx'}
        ## {"input": lambda x:x["question"]} æ¥æ”¶ {"question":"ä½ å¥½"} è¾“å‡º {"input":"ä½ å¥½"}
        ## RunnableParallel æ¥æ”¶ {"input":"ä½ å¥½"} è¾“å‡º {'question_classify_response': '', 'question_retrieval_response': '-\n-\n-\n-', 'question': 'ä½ å¥½'}
        ## question_classify_chain æ¥æ”¶ {"input":"ä½ å¥½"} è¾“å‡º '' åˆ†ç±»é“¾ç»“æœ
        ## question_retrieval_chain æ¥æ”¶ {"input":"ä½ å¥½"} è¾“å‡º '-\n-\n-\n-' æ£€ç´¢é“¾ç»“æœ
        ## RunnableLambda(lambda x:x["input"]) æ¥æ”¶ {"input":"ä½ å¥½"} è¾“å‡º "ä½ å¥½"
        ## RunnableLambda(self.generate_context_prompt) æ¥æ”¶ {'question_classify_response': '', 'question_retrieval_response': '-\n-\n-\n-', 'question': 'ä½ å¥½'} è¾“å‡º '\n\n\n\n' ä¸Šä¸‹æ–‡æç¤º 
        self.final_chain = (
            RunnablePassthrough.assign(context= {"input": lambda x:x["question"]}|\
                RunnableParallel(question_classify_response=self.question_classify_chain,
                                 question_retrieval_response=self.question_retrieval_chain,
                                 question = RunnableLambda(lambda x:x["input"]),
                                ) |\
                RunnableLambda(self.generate_context_prompt)) |\
            ChatPromptTemplate.from_messages([self.system_message_prompt, self.human_message_prompt]) | \
            self.chat | \
            StrOutputParser() 
        )
            
    
    # æ±‚æœ€é•¿å…¬å…±å­ä¸²
    def longest_common_substring(self, s1, s2):
        # è·å–ä¸¤ä¸ªå­—ç¬¦ä¸²çš„é•¿åº¦
        len_s1 = len(s1)
        len_s2 = len(s2)

        # åˆ›å»ºä¸€ä¸ªäºŒç»´æ•°ç»„ç”¨æ¥å­˜å‚¨åŠ¨æ€è§„åˆ’çš„ç»“æœ
        dp = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]

        # åˆå§‹åŒ–æœ€å¤§é•¿åº¦å’Œç»“æŸä½ç½®
        max_length = 0
        end_pos = 0

        # å¡«å……åŠ¨æ€è§„åˆ’è¡¨
        for i in range(1, len_s1 + 1):
            for j in range(1, len_s2 + 1):
                if s1[i - 1] == s2[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1] + 1
                    if dp[i][j] > max_length:
                        max_length = dp[i][j]
                        end_pos = i
                else:
                    dp[i][j] = 0

        # æå–æœ€å¤§å…¬å…±å­ä¸²
        start_pos = end_pos - max_length
        return s1[start_pos:end_pos]

    # åˆå¹¶åˆ†ç±»å’Œé—®ç­”æç¤ºä¸ºcontextæç¤º
    def generate_context_prompt(self, all_dict):
        question = all_dict["question"]
        question_classify_response = all_dict["question_classify_response"]
        question_retrieval_response = all_dict["question_retrieval_response"]
        
        if len(question_classify_response) > 0:
            question_classify_template = f"""ä½ åœ¨å›ç­”ä¸­ä½“ç°ä»¥ä¸‹å†…å®¹\n\n{question_classify_response}"""
        else:
            question_classify_template = ""
        
        if len(self.longest_common_substring(question,question_retrieval_response)) >=4:
            question_retrieval_template = f"""å·¥ä½œç»å†æœ‰ä»¥ä¸‹å†…å®¹: \n\n{question_retrieval_response}"""
        else:
            question_retrieval_template = ""
        
        return f"{question_classify_template}\n\n{question_retrieval_template}\n\n"

    def prepare_question_classify_prompt(self):
        examples = []
        for key in self.question_classify_dict:
            r_examples = self.question_classify_dict[key]["examples"]
            if len(r_examples) > 0:
                examples.extend(r_examples)

        example_prompt = PromptTemplate.from_template(
            """æ–‡æœ¬: {text}
ç±»åˆ«: {label}\n"""
        )

        label_li_str = '\n- '.join(self.question_classify_dict.keys())
        label_li_str = '\n- ' + label_li_str

        prefix = f"""ç»™å‡ºæ¯ä¸ªæ–‡æœ¬çš„ç±»åˆ«ï¼Œç±»åˆ«åªèƒ½å±äºä»¥ä¸‹åˆ—å‡ºçš„ä¸€ç§\n{label_li_str}\nå¦‚æœä¸å±äºä»¥ä¸Šç±»åˆ«ï¼Œåˆ™ç±»åˆ«åç§°ä¸ºâ€œå…¶ä»–â€ã€‚\nä¾‹å¦‚ï¼š"""
        suffix = """æ–‡æœ¬: {input}\nç±»åˆ«:"""
        return examples, example_prompt, prefix, suffix

    def label_to_response(self, label):
        label = re.sub('ç±»åˆ«: ?', '', label)
        label = label if label in self.question_classify_dict else "å…¶ä»–"
        response = self.question_classify_dict[label]["response"]
        return response


url = "https://raw.githubusercontent.com/baiziyuandyufei/langchain-self-study-tutorial/main/jl.txt"
embedding_model_name = "nomic-ai/nomic-embed-text-v1.5"
chat_model_name = "accounts/fireworks/models/llama-v3-70b-instruct"
assistant = JobSearchAssistant(url, embedding_model_name, chat_model_name)

# äººæœºäº¤äº’ç•Œé¢
# é¡µé¢å¤§æ ‡é¢˜
st.title("ä¸ªäººæ±‚èŒåŠ©æ‰‹")
st.title("ğŸ’¬ èŠå¤©æœºå™¨äºº")
# é¡µé¢æè¿°
st.caption("ğŸš€ ä¸€ä¸ªStreamlitä¸ªäººæ±‚èŒåŠ©æ‰‹èŠå¤©æœºå™¨äººï¼ŒåŸºäºFireWorksçš„llama-v3-70b-instructæ¨¡å‹")
# ä¾§è¾¹æ 
with st.sidebar:
    st.markdown("""
    ## å¼€å‘è®¡åˆ’

    ### 1. æ±‚èŒåŠ©æ‰‹

    #### ç›®çš„

    ä»£è¡¨ç”¨æˆ·åŒHRäº¤æµ

    #### å¼€å‘è®¡åˆ’

    - [x] [æ•´ä½“é“¾ç»“æ„è®¾è®¡]
    - [x] [é€šç”¨èŠå¤©æ¨¡æ¿è®¾è®¡]
    - [x] [é—®é¢˜åˆ†ç±»é“¾è®¾è®¡ï¼šé—®é¢˜åˆ†ç±»FewShotæ¨¡æ¿è®¾è®¡]
    - [x] [é—®é¢˜æ£€ç´¢é“¾è®¾è®¡ï¼šç®€å†çŸ¥è¯†åº“æ„å»º-åµŒå…¥æ¨¡å‹é€‰æ‹©ã€å‘é‡å­˜å‚¨ã€æ£€ç´¢]
    - [x] äººæœºäº¤äº’ç•Œé¢å¼€å‘
    - [x] BOSSä¸Šè‡ªåŠ¨å›å¤
    - [] æ–°JDæ£€æµ‹ä¸æ‰“æ‹›å‘¼
    - [] [è·ç¦»è®¡ç®—é“¾è®¾è®¡]
        - ç›´çº¿è·ç¦»
        - æ›¼å“ˆé¡¿è·ç¦»ï¼ˆæœ‰åœ°å›¾ä¿¡æ¯åï¼‰
    - [] [é¢è¯•ä¿¡æ¯è®°å½•é“¾]
        - è¾“å…¥æ–‡æœ¬ä¿¡æ¯ï¼ŒLLMè‡ªåŠ¨è½¬æ ¼å¼åŒ–æ•°æ®ï¼Œå…¥æœ¬åœ°æ•°æ®åº“ã€‚
        - æŠ½å–å‡ºçš„æ ¼å¼åŒ–ä¿¡æ¯åŒ…æ‹¬ï¼šå…¬å¸åç§°ï¼Œé¢è¯•æ—¶é—´ï¼Œé¢è¯•æ–¹å¼ï¼Œé¢è¯•è¯„ä»·ï¼Œé¢è¯•é—®é¢˜ã€‚
    - [ ] JDä¸æˆ‘åŒ¹é…åº¦è®¡ç®—
        - è·ç¦»æ‰“åˆ†ã€‚
        - ç›¸å…³åº¦æ‰“åˆ†ã€‚
        - è–ªèµ„æ‰“åˆ†ã€‚
        - å…¬å¸äººæ•°æ‰“åˆ†ã€‚
        - æ³¨å†Œèµ„é‡‘æ‰“åˆ†ã€‚
        - å…¬å¸æ€§è´¨æ‰“åˆ†ã€‚
        - åŠ æƒæ€»åˆ†ã€‚

    #### å½“å‰è¿›åº¦

    - è·å–åœ°ç‚¹ç»çº¬åº¦ï¼Œè®¡ç®—ç›´çº¿è·ç¦»ã€‚
    - å¬è¯´åº†ä½™å¹´äºŒå­£å‘å¸ƒäº†æ˜¯å—ï¼Ÿè¦çœ‹ã€‚

    ### 2. å¿«é€Ÿç”Ÿæˆç®€å†

    #### ç›®çš„

    æ ¹æ®ä¸åŒJDæè¿°ï¼Œç”Ÿæˆé€‚é…çš„ç®€å†å†…å®¹

    #### å¡«å†™é¡¹

    ç”¨æˆ·å¡«è¡¨ï¼Œç‚¹å‡»æäº¤åï¼Œç”Ÿæˆç®€å†ã€‚

    - å²—ä½èŒè´£ï¼š
    - å²—ä½æè¿°ï¼š
    - å­¦å†ä¿¡æ¯ï¼š
    - å·¥ä½œç»å†ï¼š
        - å¼€å‘è¯­è¨€
        - å·¥å…·åº“
        - æ¨¡å‹
        - å…¶ä»–
    - ç¦»èŒåŸå› ï¼š
    - ç°å±…ä½åœ°ï¼š

    """)

# åˆå§‹åŒ–èŠå¤©æ¶ˆæ¯ä¼šè¯
if "messages" not in st.session_state:
    #  æ·»åŠ åŠ©æ‰‹æ¶ˆæ¯
    st.session_state["messages"] = [
        {"role": "assistant", "content": "æˆ‘æ˜¯æ±‚èŒåŠ©æ‰‹ï¼Œæ›¿æˆ‘çš„ä¸»äººå›ç­”HRçš„é—®é¢˜ï¼Œä½ å¯ä»¥å°†é—®é¢˜è¾“å…¥ç»™æˆ‘ï¼"}]

# æ˜¾ç¤ºä¼šè¯ä¸­çš„æ‰€æœ‰èŠå¤©æ¶ˆæ¯
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# èŠå¤©è¾“å…¥è¡¨æ ¼
# è¿™å¥ä»£ç ä½¿ç”¨äº†æµ·è±¡è¿ç®—ç¬¦ï¼Œå°†ç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥çš„å†…å®¹èµ‹å€¼ç»™å˜é‡promptï¼Œå¹¶æ£€æŸ¥è¿™ä¸ªè¾“å…¥å†…å®¹æ˜¯å¦ä¸ºçœŸï¼ˆå³æ˜¯å¦æœ‰è¾“å…¥å†…å®¹ï¼‰ã€‚
if prompt := st.chat_input("HRçš„é—®é¢˜"):
    logger.info(f"ç”¨æˆ·è¾“å…¥: {prompt}")
    # å‘ä¼šè¯æ¶ˆæ¯ä¸­æ·»åŠ ç”¨æˆ·è¾“å…¥
    st.session_state.messages.append({"role": "user", "content": prompt})
    # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.chat_message("user").write(prompt)
    # è°ƒç”¨é“¾è·å–å“åº”
    response = assistant.final_chain.invoke({"question":prompt})
    logger.info(f"AIå“åº”: {response}")
    # å‘ä¼šè¯æ¶ˆæ¯ä¸­æ·»åŠ åŠ©æ‰‹è¾“å…¥
    st.session_state.messages.append(
        {"role": "assistant", "content": response})
    # æ˜¾ç¤ºåŠ©æ‰‹æ¶ˆæ¯
    st.chat_message("assistant").write(response)
