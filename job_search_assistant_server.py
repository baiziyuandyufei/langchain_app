import streamlit as st
from dotenv import load_dotenv
import logging
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.document_loaders import WebBaseLoader
from langchain_groq import ChatGroq
from langchain_fireworks import FireworksEmbeddings, ChatFireworks
from langchain.vectorstores import FAISS
from langchain_core.prompts import (
    FewShotPromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
    ChatPromptTemplate,
    PromptTemplate)
from langchain_core.runnables import (
    RunnableParallel,
    RunnablePassthrough,
    RunnableLambda)
from langchain_core.output_parsers import StrOutputParser
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
import re


class JobSearchAssistant:
    def __init__(self, url, embedding_model_name, chat_model_name):
        # 文档加载、分割
        self.loader = WebBaseLoader(url)
        self.raw_documents = self.loader.load()
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=100,
            chunk_overlap=20,
            length_function=len,
            add_start_index=False,
        )
        self.documents = self.text_splitter.split_documents(self.raw_documents)
        print(f"分割后快数: {len(self.documents)}")

        # 向量化、存储
        if embedding_model_name == "BAAI/bge-large-zh-v1.5":
            model_kwargs = {"device": "cpu"}
            encode_kwargs = {"normalize_embeddings": True}
            self.embedding_model = HuggingFaceBgeEmbeddings(
                model_name=embedding_model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs
            )
        else:
            self.embedding_model = FireworksEmbeddings(model=embedding_model_name)
        self.db = FAISS.from_documents(
            documents=self.documents, embedding=self.embedding_model)
        print(f"索引片段数: {self.db.index.ntotal}")

        # 检索器
        self.retriever = self.db.as_retriever()

        # 检索链
        self.question_retrieval_chain = RunnableLambda(lambda x:x["input"]) | self.retriever | RunnableLambda(
            lambda docs: "\n".join([doc.page_content for doc in docs]))

        # 实例化聊天模型
        if chat_model_name == "llama3-70b-8192":
            self.chat = ChatGroq(
                temperature=0.3,
                model=chat_model_name,
                )
        else:
            self.chat = ChatFireworks(
                model=chat_model_name,
                temperature=0.3,
                model_kwargs={
                    "top_p":0.3
                    }
                )

        # 分类词典
        self.question_classify_dict = {
            "外包&外协&外派&驻场": {
                "response": "不考虑外包&外协&外派&驻场&兼职，不接受出差。",
                "examples": [{"text": "你好，我们是外协岗位，在国家电网 南瑞工作的", "label": "外包&外协&外派&驻场"}]
            },
            "兼职": {
                "response": "不考虑不考虑外包&外协&外派&驻场&兼职，不接受出差。",
                "examples": [{"text": "哈喽～本职位为线上兼职，一单一结款，根据自己时间自由接单，不耽误自己的主业，您看感兴趣嘛？", "label": "兼职"}]
            },
            "预约面试&面试时间&空闲时间": {
                "response": "稍等我确定一下时间。",
                "examples": [{"text": "想约您面试，方便的话麻烦告诉我一下您可以约面试的日期及时间。", "label": "预约面试&面试时间&空闲时间"},
                             {"text": "明天你看看什么时间方便。", "label": "预约面试&面试时间&空闲时间"},
                             {"text": "那就明天下午14点吧。", "label": "预约面试&面试时间&空闲时间"}]
            },
            "会议邀请": {
                "response": "稍等我确定一下时间。",
                "examples": [{"text": "会议主题：中电信数智科技会议时间：中国标准时间 - 北京点击链接入会，，打开手机腾讯会议即可参与", "label": "会议邀请"}]
            },
            "简历": {
                "response": "请认真阅读并回答职位是否为外包&外协&外派&驻场&兼职？是否出差？之后我才会发送简历",
                "examples": [{"text": "您好 方便发一份简历过来吗？", "label": "简历"}]
            },
            "请求微信": {
                "response": "面试时再加微信",
                "examples": [{"text": "你好 方便加个微信吗？", "label": "请求微信"}]
            },
            "其他": {
                "response": "",
                "examples": []
            }
        }

        # 构建分类提示
        self.examples, self.example_prompt, self.prefix, self.suffix = self.prepare_question_classify_prompt()
        self.few_shot_prompt = FewShotPromptTemplate(
            examples=self.examples,
            example_prompt=self.example_prompt,
            prefix=self.prefix,
            suffix=self.suffix,
            input_variables=["input"],
            example_separator="\n"
        )

        # 分类链
        self.question_classify_chain = self.few_shot_prompt | self.chat | StrOutputParser(
        ) | RunnableLambda(self.label_to_response)

        # 系统提示
        self.system_message_prompt = SystemMessagePromptTemplate.from_template(f"""
        你是一个求职助手，代替应聘者，回复HR的问题。
        
        你在回复时，称呼对方为"HR经理"开头。
        
        以下是应聘者的信息，以下这些信息只有被问到时才输出。
        ```
        应聘者的姓名：于先生。
        工作经历：
        - 练手项目 2024年5月至今 担任自然语言处理工程师 工作内容：基于LangChain的求职助手，基于llama3-70B模型，实现HR问题分类、简历检索、职位匹配度计算。
        - 信工所   2018年8月至2024年5月 担任自然语言处理工程师 工作内容：负责短文本分类、实体识别，使用pyhton/Java语言，使用Transformer PyTorch实现基于Bi-LSTM-CRF模型的实体识别。 离职。
        - 苏宁易购 2017年8月至2018年7月 担任自然语言处理工程师 工作内容：负责商品标题分类和属性词抽取，使用C/C++语言。 离职。
        - 同方知网 2015年3月至2017年7月 担任计算语言学工程师 工作内容：负责论文抄袭检测算法设计实现、OpenCV相似图像检索，使用C/C++语言，MySQL数据库。 离职。
        现居住地：北京。
        期望工作地：北京，不接受出差。
        教育背景：北京信息科技大学 硕士/本科。
        联系方式：面试时再互相添加联系方式。
        期望职位：自然语言处理(NLP)、大模型、爬虫工程师。
        英语等级：CET-6。
        期望薪资：20-25K。
        目前薪资：20K*12。
        最快到岗时间：一周内到岗。
        离职原因：通勤不便。
        ```
        """)

        # 人工提示
        self.human_message_prompt = HumanMessagePromptTemplate.from_template("""HR问或说: {question}。\n\n{context}\n\n请用汉语回复内容，内容的头部和尾部不要出现引号。""")

        # 整体链
        ## 输入的字典的变量名叫 question
        ## RunnablePassthrough 接收输入字典{"question":"你好"} 输出 {'question': '你好', 'context':'xxx'}
        ## {"input": lambda x:x["question"]} 接收 {"question":"你好"} 输出 {"input":"你好"}
        ## RunnableParallel 接收 {"input":"你好"} 输出 {'question_classify_response': '', 'question_retrieval_response': '-\n-\n-\n-', 'question': '你好'}
        ## question_classify_chain 接收 {"input":"你好"} 输出 '' 分类链结果
        ## question_retrieval_chain 接收 {"input":"你好"} 输出 '-\n-\n-\n-' 检索链结果
        ## RunnableLambda(lambda x:x["input"]) 接收 {"input":"你好"} 输出 "你好"
        ## RunnableLambda(self.generate_context_prompt) 接收 {'question_classify_response': '', 'question_retrieval_response': '-\n-\n-\n-', 'question': '你好'} 输出 '\n\n\n\n' 上下文提示 
        self.final_chain = (
            RunnablePassthrough.assign(context= {"input": lambda x:x["question"]}|\
                RunnableParallel(question_classify_response=self.question_classify_chain,
                                 question_retrieval_response=self.question_retrieval_chain,
                                 question = RunnableLambda(lambda x:x["input"]),
                                ) |\
                RunnableLambda(self.generate_context_prompt)) |\
            ChatPromptTemplate.from_messages([self.system_message_prompt, self.human_message_prompt]) | \
            self.chat | \
            StrOutputParser()
        )

    # 求最长公共子串
    def longest_common_substring(self, s1, s2):
        # 获取两个字符串的长度
        len_s1 = len(s1)
        len_s2 = len(s2)

        # 创建一个二维数组用来存储动态规划的结果
        dp = [[0] * (len_s2 + 1) for _ in range(len_s1 + 1)]

        # 初始化最大长度和结束位置
        max_length = 0
        end_pos = 0

        # 填充动态规划表
        for i in range(1, len_s1 + 1):
            for j in range(1, len_s2 + 1):
                if s1[i - 1] == s2[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1] + 1
                    if dp[i][j] > max_length:
                        max_length = dp[i][j]
                        end_pos = i
                else:
                    dp[i][j] = 0

        # 提取最大公共子串
        start_pos = end_pos - max_length
        return s1[start_pos:end_pos]

    # 合并分类和问答提示为context提示
    def generate_context_prompt(self, all_dict):
        question = all_dict["question"]
        question_classify_response = all_dict["question_classify_response"]
        question_retrieval_response = all_dict["question_retrieval_response"]

        if len(question_classify_response) > 0:
            question_classify_template = f"""你在回答中体现以下内容\n\n{question_classify_response}"""
        else:
            question_classify_template = ""

        if len(self.longest_common_substring(question,question_retrieval_response)) >=4:
            question_retrieval_template = f"""工作经历有以下内容: \n\n{question_retrieval_response}"""
        else:
            question_retrieval_template = ""

        return f"{question_classify_template}\n\n{question_retrieval_template}\n\n"

    def prepare_question_classify_prompt(self):
        examples = []
        for key in self.question_classify_dict:
            r_examples = self.question_classify_dict[key]["examples"]
            if len(r_examples) > 0:
                examples.extend(r_examples)

        example_prompt = PromptTemplate.from_template(
            """文本: {text}
类别: {label}\n"""
        )

        label_li_str = '\n- '.join(self.question_classify_dict.keys())
        label_li_str = '\n- ' + label_li_str

        prefix = f"""给出每个文本的类别，类别只能属于以下列出的一种\n{label_li_str}\n如果不属于以上类别，则类别名称为“其他”。\n例如："""
        suffix = """文本: {input}\n类别:"""
        return examples, example_prompt, prefix, suffix

    def label_to_response(self, label):
        label = re.sub('类别: ?', '', label)
        label = label if label in self.question_classify_dict else "其他"
        response = self.question_classify_dict[label]["response"]
        return response


if __name__ == "__main__":
    # 配置日志
    logging.basicConfig(
        format='%(asctime)s %(levelname)s %(message)s',
        level=logging.INFO,
        handlers=[
            logging.StreamHandler()
        ]
    )
    # 获取日志记录器
    logger = logging.getLogger(__name__)
    # 部署到streamlit时，请在streamlit中配置环境变量
    load_dotenv()
    os.environ["LANGCHAIN_TRACING_V2"] = "true"
    os.environ["LANGCHAIN_PROJECT"] = "job-search-assistant-webpage"
    logging.info(f'LANGCHAIN_TRACING_V2: {os.getenv("LANGCHAIN_TRACING_V2", "未设置")}')
    logging.info(f'LANGCHAIN_ENDPOINT: {os.getenv("LANGCHAIN_ENDPOINT", "未设置")}')
    logging.info(f'LANGCHAIN_PROJECT: {os.getenv("LANGCHAIN_PROJECT", "未设置")}')

    url = "https://raw.githubusercontent.com/baiziyuandyufei/langchain-self-study-tutorial/main/jl.txt"
    # embedding_model_name = "nomic-ai/nomic-embed-text-v1.5"
    embedding_model_name = "BAAI/bge-large-zh-v1.5"
    # chat_model_name = "accounts/fireworks/models/llama-v3-70b-instruct"
    chat_model_name = "llama3-70b-8192"
    assistant = JobSearchAssistant(url, embedding_model_name, chat_model_name)

    # 人机交互界面
    # 页面大标题
    st.title("个人求职助手")
    st.title("💬 聊天机器人")
    # 页面描述
    st.caption("🚀 一个Streamlit个人求职助手聊天机器人，基于FireWorks的llama-v3-70b-instruct模型")
    # 侧边栏
    with st.sidebar:
        st.markdown("""
        ## 开发计划

        ### 1. 求职助手

        #### 目的

        代表用户同HR交流

        #### 开发计划

        - [x] [整体链结构设计]
        - [x] [通用聊天模板设计]
        - [x] [问题分类链设计：问题分类FewShot模板设计]
        - [x] [问题检索链设计：简历知识库构建-嵌入模型选择、向量存储、检索]
        - [x] 人机交互界面开发
        - [x] BOSS上自动回复
        - [ ] 新JD检测与打招呼
        - [ ] [距离计算链设计]
            - 直线距离
            - 曼哈顿距离（有地图信息后）
        - [ ] [面试信息记录链]
            - 输入文本信息，LLM自动转格式化数据，入本地数据库。
            - 抽取出的格式化信息包括：公司名称，面试时间，面试方式，面试评价，面试问题。
        - [ ] JD与我匹配度计算
            - 距离打分。
            - 相关度打分。
            - 薪资打分。
            - 公司人数打分。
            - 注册资金打分。
            - 公司性质打分。
            - 加权总分。

        #### 当前进度

        - 获取地点经纬度，计算直线距离。
        - 听说庆余年二季发布了是吗？要看。

        ### 2. 快速生成简历

        #### 目的

        根据不同JD描述，生成适配的简历内容

        #### 填写项

        用户填表，点击提交后，生成简历。

        - 岗位职责：
        - 岗位描述：
        - 学历信息：
        - 工作经历：
            - 开发语言
            - 工具库
            - 模型
            - 其他
        - 离职原因：
        - 现居住地：

        """)

    # 初始化聊天消息会话
    if "messages" not in st.session_state:
        #  添加助手消息
        st.session_state["messages"] = [
            {"role": "assistant", "content": "我是求职助手，替我的主人回答HR的问题，你可以将问题输入给我！"}]

    # 显示会话中的所有聊天消息
    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # 聊天输入表格
    # 这句代码使用了海象运算符，将用户在聊天输入框中输入的内容赋值给变量prompt，并检查这个输入内容是否为真（即是否有输入内容）。
    if prompt := st.chat_input("HR的问题"):
        logger.info(f"用户输入: {prompt}")
        # 向会话消息中添加用户输入
        st.session_state.messages.append({"role": "user", "content": prompt})
        # 显示用户输入
        st.chat_message("user").write(prompt)
        # 调用链获取响应
        response = assistant.final_chain.invoke({"question":prompt})
        logger.info(f"AI响应: {response}")
        # 向会话消息中添加助手输入
        st.session_state.messages.append(
            {"role": "assistant", "content": response})
        # 显示助手消息
        st.chat_message("assistant").write(response)
